{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkshaySaini25/Cardiovascular_risk_predeiction/blob/main/cardiovascular_risk_ML_Submission_Temp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vncDsAP0Gaoa"
      },
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beRrZCGUAJYm"
      },
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual/Team\n",
        "##### **Team Member 1 -**\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJNUwmbgGyua"
      },
      "source": [
        "# **Project Summary -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6v_1wHtG2nS"
      },
      "source": [
        "Write the summary here within 500-600 words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6K7xa23Elo4"
      },
      "source": [
        "# **GitHub Link -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1o69JH3Eqqn"
      },
      "source": [
        "Provide your GitHub Link here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQaldy8SH6Dl"
      },
      "source": [
        "# **Problem Statement**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpeJGUA3kjGy"
      },
      "source": [
        "**Write Problem Statement Here.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDgbUHAGgjLW"
      },
      "source": [
        "# **General Guidelines** : -  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      },
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_i_v8NEhb9l"
      },
      "source": [
        "# ***Let's Begin !***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhfV-JJviCcP"
      },
      "source": [
        "## ***1. Know Your Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3lxredqlCYt"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "outputs": [],
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIMwtI0X7q1H"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt  # Matplotlib for plotting\n",
        "%matplotlib inline\n",
        "import pylab\n",
        "import seaborn as sns  # Seaborn for advanced data visualization\n",
        "\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler  # StandardScaler for feature scaling\n",
        "from sklearn.model_selection import train_test_split  # train_test_split for data splitting\n",
        "from imblearn.over_sampling import SMOTE  # SMOTE for oversampling\n",
        "from sklearn.linear_model import LogisticRegression  # Logistic Regression classifier\n",
        "from sklearn.tree import DecisionTreeClassifier  # Decision Tree classifier\n",
        "from sklearn.ensemble import RandomForestClassifier  # Random Forest classifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier  # Gradient Boosting classifier\n",
        "from sklearn.ensemble import AdaBoostClassifier  # AdaBoost classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier  # K-Nearest Neighbors classifier\n",
        "from sklearn.naive_bayes import GaussianNB  # Gaussian Naive Bayes classifier\n",
        "from xgboost import XGBClassifier  # XGBoost classifier\n",
        "from sklearn.svm import SVC  # Support Vector Machine classifier\n",
        "\n",
        "\n",
        "## Importing essential libraries to check the accuracy\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score  # Metrics for evaluating classification accuracy\n",
        "from sklearn.metrics import roc_auc_score, classification_report  # Metrics for ROC AUC and classification report\n",
        "from sklearn.metrics import roc_curve, auc  # ROC curve and AUC calculation\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score  # Precision, Recall, and F1 score metrics\n",
        "from sklearn.model_selection import GridSearchCV  # GridSearchCV for hyperparameter tuning\n",
        "from sklearn.model_selection import RandomizedSearchCV  # RandomizedSearchCV for hyperparameter tuning\n",
        "\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wH5WlXm7q1H"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RnN4peoiCZX"
      },
      "source": [
        "### Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMFEjTN07q1H"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/capston project/cardiovascular_risk_ML/data_cardiovascular_risk.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3C1gYSP7q1I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArB3WqKK7q1I"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x71ZqKXriCWQ"
      },
      "source": [
        "### Dataset First View"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "outputs": [],
      "source": [
        "# Dataset First Look"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IV62P6hs7q1I"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ff_s4wgN7q1I"
      },
      "outputs": [],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hBIi_osiCS2"
      },
      "source": [
        "### Dataset Rows & Columns count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "outputs": [],
      "source": [
        "# Dataset Rows & Columns count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxbBhsak7q1J"
      },
      "outputs": [],
      "source": [
        "#Checking number of rows and columns in our dataset.\n",
        "print(\"Number of rows is: \", df.shape[0])\n",
        "print(\"Number of columns is: \", df.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlHwYmJAmNHm"
      },
      "source": [
        "### Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "outputs": [],
      "source": [
        "# Dataset Info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHh_C1Ph7q1J"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "8AGSkm9w7q1K"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZCR2aGb7q1K"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVoVLGvF7q1K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35m5QtbWiB9F"
      },
      "source": [
        "#### Duplicate Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "outputs": [],
      "source": [
        "# Dataset Duplicate Value Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4If6PBg7q1K"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoPl-ycgm1ru"
      },
      "source": [
        "#### Missing Values/Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "outputs": [],
      "source": [
        "# Missing Values/Null Values Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "kJrgXG6-7q1Q"
      },
      "outputs": [],
      "source": [
        "for i in df.columns.tolist():\n",
        "    print(\"Total missing Values in\",i,\":\",df[i].isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "DiDdTQYp7q1Q"
      },
      "outputs": [],
      "source": [
        "#checking duplicate rows in our dataset\n",
        "len(df.duplicated())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "outputs": [],
      "source": [
        "# Visualizing the missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7-avelN7q1Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0kj-8xxnORC"
      },
      "source": [
        "### What did you know about your dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfoNAAC-nUe_"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      },
      "source": [
        "## ***2. Understanding Your Variables***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "outputs": [],
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "outputs": [],
      "source": [
        "# Dataset Describe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBTbrJXOngz2"
      },
      "source": [
        "### Variables Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mks33SWY7q1R"
      },
      "source": [
        "Demographics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8haW3wX7q1S"
      },
      "source": [
        "Sex: male or female(\"M\" or \"F\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ATZbP3B7q1S"
      },
      "source": [
        "Age: Age of the patient;(Continous- Althrough the recorded ages have been truncated to whole number, the concept of age us continous)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnjvkE7c7q1S"
      },
      "source": [
        "Education"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYREarZc7q1S"
      },
      "source": [
        "Behavioural\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNPLe5Qa7q1S"
      },
      "source": [
        "is_smoking: weather the person smokes or not(\"YES\" or \"NO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pCOuyaP7q1S"
      },
      "source": [
        "cigsPerDay: the number of cigarettes that ht eperson smoked on average in one day"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Clcv4DvV7q1S"
      },
      "source": [
        "Medical (history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgCY_vLK7q1T"
      },
      "source": [
        "BPMeds: whether or not the patient was on blood pressure medication\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSKnYClS7q1T"
      },
      "source": [
        "PrevalentStroke: weather or not the patient had previously had a stroke"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Apm_8eS7q1T"
      },
      "source": [
        "PrevalentHyp: weather or not the patient was hypertensive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kq2Ajmn7q1T"
      },
      "source": [
        "Diabetes: weather or ot the patient had diabetes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYNDqFmn7q1T"
      },
      "source": [
        "Medical(current)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZcNSNz97q1T"
      },
      "source": [
        "Tot Chol: total cholesterol level (Continuous)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nARotakI7q1T"
      },
      "source": [
        "Sys BP: systolic blood pressure (Continuous)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaU9n_Fb7q1U"
      },
      "source": [
        " Dia BP: diastolic blood pressure (Continuous)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfN6QSjk7q1U"
      },
      "source": [
        "BMI: Body Mass Index (Continuous)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xcb-mzR27q1U"
      },
      "source": [
        "Heart Rate:* heart rate (Continuous - In medical research, variables such as heart rate though in fact discrete, yet are considered continuous because of large number of possible values.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgxVFCU77q1U"
      },
      "source": [
        "Glucose:* glucose level (Continuous)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV8ugH0f7q1U"
      },
      "source": [
        "Predict Variable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFI_DAZV7q1U"
      },
      "source": [
        "TenYearCHD: 10-year risk of coronary heart disease CHD(binary: “1”, means “Yes” , “0” means “No”) - DEPENDANT VARIABLE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4s0lvMU7q1V"
      },
      "source": [
        "['id', 'age', 'education', 'sex', 'is_smoking', 'cigsPerDay', 'BPMeds',\n",
        "       'prevalentStroke', 'prevalentHyp', 'diabetes', 'totChol', 'sysBP',\n",
        "       'diaBP', 'BMI', 'heartRate', 'glucose', 'TenYearCHD']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJGvrvIx7q1V"
      },
      "outputs": [],
      "source": [
        "# Dataset Describe    (used to get statistics of numerical columns)\n",
        "df.describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3PMJOP6ngxN"
      },
      "source": [
        "Check Unique Values for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "outputs": [],
      "source": [
        "# Check Unique Values for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqgh0JpK7q1V"
      },
      "outputs": [],
      "source": [
        "for i in df.columns.tolist():\n",
        "    print(\"Total unique values in\",i,\":\",len(pd.unique(df[i])))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h-5m-aY3U7NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dauF4eBmngu3"
      },
      "source": [
        "## 3.***Data Wrangling***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJF3rekwFvQ"
      },
      "source": [
        "#### Data Wrangling Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "outputs": [],
      "source": [
        "# Write your code to make your dataset analysis ready."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-i4TWQP7q1W"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Renaming the columns\n",
        "df.rename(columns={'cigsPerDay':'cigs_per_day','BPMeds':'bp_meds',\n",
        "                   'prevalentStroke':'prevalent_stroke','prevalentHyp':'prevalent_hyp',\n",
        "                   'totChol':'total_cholesterol','sysBP':'systolic_bp','diaBP':'diastolic_bp',\n",
        "                   'BMI':'bmi','heartRate':'heart_rate','TenYearCHD':'ten_year_chd'},\n",
        "          inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72Gcwo1m7q1W"
      },
      "outputs": [],
      "source": [
        "#Defining columns into three types of list\n",
        "# a. dependent variable\n",
        "# b. continuous  independent variable\n",
        "# c. categorical independent variable\n",
        "\n",
        "dependent_var=['ten_year_chd']\n",
        "continuous_var=   ['age','cigs_per_day','total_cholesterol','systolic_bp', 'diastolic_bp', 'bmi', 'heart_rate', 'glucose']\n",
        "categorical_var = ['education', 'sex', 'is_smoking','bp_meds','prevalent_stroke', 'prevalent_hyp', 'diabetes']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewRlji-L7q1W"
      },
      "outputs": [],
      "source": [
        "df['sex']=np.where(df['sex']=='M',1,0)\n",
        "df['is_smoking']=np.where(df['is_smoking']=='YES',1,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NshsaYu77q1W"
      },
      "outputs": [],
      "source": [
        "# sum of the null value\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbKYfpTD7q1X"
      },
      "source": [
        "Handling the missing values is one of the greatest challenges faced by analysts, because making the right decision on how to handle it generates robust data models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqtQkAM_7q1X"
      },
      "source": [
        "###### Let us look at different ways of imputing the missing values\n",
        "\n",
        "1. Deleting Rows.\n",
        "2. Replacing With Mean/Median/Mode\n",
        "3. Assigning An Unique Category\n",
        "4. Assigning An Unique Category\n",
        "\n",
        "\n",
        "Will be using the 1st and 2nd Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6lLzZe17q1X"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-XAgyMBhzIw"
      },
      "source": [
        "#### 3.1 Checking Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuVHFl9PhzIw"
      },
      "outputs": [],
      "source": [
        "# sum of the null value\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EFz9K2HhzIw"
      },
      "source": [
        "Handling the missing values is one of the greatest challenges faced by analysts, because making the right decision on how to handle it generates robust data models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNePmjNKhzIw"
      },
      "source": [
        "###### Let us look at different ways of imputing the missing values\n",
        "\n",
        "1. Deleting Rows.\n",
        "2. Replacing With Mean/Median/Mode\n",
        "3. Assigning An Unique Category\n",
        "4. Assigning An Unique Category\n",
        "\n",
        "\n",
        "Will be using the 1st and 2nd Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19zixgPGhzIw"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3Q9yoXEhzIw"
      },
      "source": [
        "#### 3.1.1 Handling Missing Categorical values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUxCczLZhzIx"
      },
      "outputs": [],
      "source": [
        "df['education']=df['education'].fillna(df['education'].mode()[0])\n",
        "df['bp_meds']=df['bp_meds'].fillna(df['bp_meds'].mode()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsRnCG9hhzIx"
      },
      "outputs": [],
      "source": [
        "#education distribution after mode imputation\n",
        "df.education.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "R53vYC_ThzIx"
      },
      "outputs": [],
      "source": [
        "#bp_meds distribution after mode imputation\n",
        "df.bp_meds.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hd-NHKIhzIx"
      },
      "source": [
        "#### 3.1.2 Handling Missing Continous values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dO6s7S4YhzIx"
      },
      "outputs": [],
      "source": [
        "#Mean and Median number of cigarettes per day\n",
        "round(df.cigs_per_day.mean(),0),df.cigs_per_day.median()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Eqvt0SJrhzIx"
      },
      "outputs": [],
      "source": [
        "#All missing Values in the cigs_per_day column\n",
        "df[df['cigs_per_day'].isna()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On_TAF0zhzIx"
      },
      "source": [
        "##### From the ablove table , we find that for every instance of missing value in cigs_per_dat , the patient reported that they smoke"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6iQDPGChzIy"
      },
      "source": [
        "Let's check the mean and median number of cigarettes smoked by patient , who reported that they smoke"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQiOYA00hzIy"
      },
      "outputs": [],
      "source": [
        "df[df['is_smoking']==1]['cigs_per_day'].mean(),df[df['is_smoking']==1]['cigs_per_day'].median()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "cQB7nFjkhzIz"
      },
      "outputs": [],
      "source": [
        "# distribution of number of cigarettes per day for smokers (excluding non-smokers)\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot(df[df['is_smoking']==1]['cigs_per_day'])\n",
        "plt.axvline(df[df['is_smoking']==1]['cigs_per_day'].mean(),color='magenta',linestyle='dashed')\n",
        "plt.axvline(df[df['is_smoking']==1]['cigs_per_day'].median(),color='cyan',linestyle='dashed')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP5qsYZyhzIz"
      },
      "outputs": [],
      "source": [
        "#box plot for the number of cigs per day for the smokers(excluding non-smokers)\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.boxplot(df[df['is_smoking']==1]['cigs_per_day'],orient='h')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi_rXyFhhzIz"
      },
      "source": [
        "##### Since the number of cigarettes smoked by the patients who smoke contains outliers, the missing values in ths cigs_per_day column can be imputed with its median value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0lA5mEnhzIz"
      },
      "outputs": [],
      "source": [
        "df['cigs_per_day']=df['cigs_per_day'].fillna(df[df['is_smoking']==1]['cigs_per_day'].median())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0aqnGRRhzIz"
      },
      "outputs": [],
      "source": [
        "# Checking for any wrong entries where the patient is not a smoker\n",
        "# and cigarettes per day above 0\n",
        "\n",
        "df[(df['is_smoking']==0) & (df['cigs_per_day']>0)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8_jiznRvjASV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbKrihe1hzIz"
      },
      "source": [
        "#### Handling total_cholesterol, bmi, heart_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pXn5fknhzIz"
      },
      "outputs": [],
      "source": [
        "# checking distribution of Handling total_cholestrol, bmi, heart_rate\n",
        "for i in ['total_cholesterol','bmi', 'heart_rate']:\n",
        "    plt.figure(figsize=(10,5))\n",
        "    sns.distplot(df[i])\n",
        "    plt.axvline(df[i].mean(),color='magenta',linestyle='dashed',linewidth=2)\n",
        "    plt.axvline(df[i].median(), color='cyan', linestyle='dashed', linewidth=2)\n",
        "    plt.title(i+' distribution')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukV7JiANhzI0"
      },
      "outputs": [],
      "source": [
        "for i in ['total_cholesterol','bmi', 'heart_rate']:\n",
        "    plt.figure(figsize=(10,5))\n",
        "    sns.boxplot(df[i],orient='h')\n",
        "    plt.title(i+' boxplot')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE9m4tYNhzI0"
      },
      "source": [
        "#### the total_cholestrol, bmi, and heart_rate columns contail outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1afbDaChzI0"
      },
      "outputs": [],
      "source": [
        "# Mean and medium  for total_cholestrol\n",
        "df.total_cholesterol.mean(),df['total_cholesterol'].median()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvHaX_Q_hzI0"
      },
      "outputs": [],
      "source": [
        "#mean and median for bmi\n",
        "df['bmi'].mean(),df['bmi'].median()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pE1luq5qhzI0"
      },
      "outputs": [],
      "source": [
        "#mean and median for heart_rate\n",
        "df.heart_rate.mean(),df.heart_rate.median()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mZUwhP-hzI0"
      },
      "outputs": [],
      "source": [
        "# imputing missing values in total_cholesterol, bmi and heart_rate with median\n",
        "df['total_cholesterol']=df['total_cholesterol'].fillna(df['total_cholesterol'].median())\n",
        "df['bmi']=df['bmi'].fillna(df['bmi'].median())\n",
        "df['heart_rate']=df['heart_rate'].fillna(df['heart_rate'].median())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZ_AXIIXhzI0"
      },
      "outputs": [],
      "source": [
        "# sum of the null value\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isqIuNOEhzI0"
      },
      "source": [
        "####  Handling glucose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1JyNfSKhzI0"
      },
      "source": [
        "#### glucose column contain 304 missing value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIjWTZgGhzI1"
      },
      "outputs": [],
      "source": [
        "# distribution of the glucose\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot(df['glucose'])\n",
        "plt.axvline(df['glucose'].mean(),color='magenta',linestyle='dashed',linewidth=2)\n",
        "plt.axvline(df['glucose'].median(), color='cyan', linestyle='dashed', linewidth=2)\n",
        "plt.title('Glucose Distibution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re47_QOuhzI1"
      },
      "source": [
        "#### The Glucose column is positively skewed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eHLgEe5hzI1"
      },
      "outputs": [],
      "source": [
        "#outlire in glucose\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.boxplot(df['glucose'],orient='h')\n",
        "plt.title('Glucose Boxplot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZDVUruzhzI1"
      },
      "outputs": [],
      "source": [
        "#Mean, Median and mode for glucose\n",
        "df.glucose.mean(),df.glucose.median(),df.glucose.mode()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szRD5qjuhzI1"
      },
      "source": [
        "The Distribution is positively skewed, with outliers\n",
        "There are 304 missing value in the glucose column.\n",
        "#### If we choose to impute them with a single value of mean/ median , we will adding high bais at that point.\n",
        "#### To avoid this we can impute the missing values using KNN imputer.\n",
        "If the dataset in questio  had been a time seies, we could have used the interpolation method to impute the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLcQyzlJhzI1"
      },
      "outputs": [],
      "source": [
        "imputer = KNNImputer(n_neighbors=10)\n",
        "imputed = imputer.fit_transform(df)\n",
        "df=pd.DataFrame(imputed,columns=df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Cbdb1RXhzI1"
      },
      "outputs": [],
      "source": [
        "df.glucose.mean(),df.glucose.median(),df.glucose.mode()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6vqTozKhzI1"
      },
      "outputs": [],
      "source": [
        "# changing datatypes\n",
        "df = df.astype({'age': int, 'education':int,'sex':int,'is_smoking':int,'cigs_per_day':int,\n",
        "               'bp_meds':int,'prevalent_stroke':int,'prevalent_hyp':int,'diabetes':int,\n",
        "               'total_cholesterol':float,'systolic_bp':float,'diastolic_bp':float,\n",
        "               'bmi':float,'heart_rate':float,'glucose':float,'ten_year_chd':int})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNp5rVrJhzI2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# checking for missing values\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP4FUcWbhzI2"
      },
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2udYFCrxhzI2"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F-cwmxu2hxo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfUpvioZ7q1a"
      },
      "source": [
        "#### Handling glucose"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns\n",
        "\n"
      ],
      "metadata": {
        "id": "3b7Rt7H_WGEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF8Ens_Soomf"
      },
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wOQAZs5pc--"
      },
      "source": [
        "#### Chart - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "outputs": [],
      "source": [
        "# Chart - 1 visualization code\n",
        "# Distribution of dependent varaible - ten_year_chd\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(data=df,x='ten_year_chd')\n",
        "plt.xlabel(dependent_var[0])\n",
        "plt.title(dependent_var[0]+' distribution')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5QZ13OEpz2H"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XESiWehPqBRc"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_j1G7yiqdRP"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "448CDAPjqfQr"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cspy4FjqxJW"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSlN3yHqYklG"
      },
      "source": [
        "#### Chart - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "outputs": [],
      "source": [
        "# Chart - 2 visualization code\n",
        "for col in continuous_var:\n",
        "    plt.figure(figsize=(10,5))\n",
        "    sns.distplot(df[col])\n",
        "    plt.axvline(df[col].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "    plt.axvline(df[col].median(), color='cyan', linestyle='dashed', linewidth=2)\n",
        "    plt.title(col+' distribution')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6dVpIINYklI"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aaW0BYyYklI"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijmpgYnKYklI"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSx9atu2YklI"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JiQyfWJYklI"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcBbebzrYklV"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM7whBJCYoAo"
      },
      "source": [
        "#### Chart - 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "outputs": [],
      "source": [
        "# Chart - 3 visualization code\n",
        "for i in categorical_var:\n",
        "    # Distribution of dependent varaible - ten_year_chd\n",
        "    plt.figure(figsize=(10,5))\n",
        "    p=sns.countplot(data=df,x=i)\n",
        "\n",
        "    plt.xlabel(i)\n",
        "    plt.title(' distribution')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fge-S5ZAYoAp"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dBItgRVYoAp"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85gYPyotYoAp"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jstXR6OYoAp"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoGjAbkUYoAp"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Of9eVA-YrdM"
      },
      "source": [
        "#### Chart - 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "outputs": [],
      "source": [
        "# Chart - 4 visualization code\n",
        "\n",
        "# Relationship between the dependent variable and continuous independent variables\n",
        "for i in continuous_var:\n",
        "  plt.figure(figsize=(10,5))\n",
        "  sns.catplot(x=dependent_var[0],y=i,data=df,kind='violin')\n",
        "  plt.ylabel(i)\n",
        "  plt.xlabel(dependent_var[0])\n",
        "  plt.title(dependent_var[0]+' vs '+i)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iky9q4vBYrdO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJRCwT6DYrdO"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6T5p64dYrdO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-Ehk30pYrdP"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLNxxz7MYrdP"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bamQiAODYuh1"
      },
      "source": [
        "#### Chart - 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "outputs": [],
      "source": [
        "# Chart - 5 visualization code\n",
        "for i in categorical_var:\n",
        "  plt.figure(figsize=(10,5))\n",
        "  sns.histplot(x=i,hue=dependent_var[0],data=df,multiple='stack')\n",
        "  plt.title('Rishi of CHD by :'+i)\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcxuIMRPYuh3"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwzvFGzlYuh3"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyqkiB8YYuh3"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYpmQ266Yuh3"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH-pJp9IphqM"
      },
      "source": [
        "#### Chart - 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "outputs": [],
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "for i in categorical_var:\n",
        "    x_var, y_var = i, dependent_var[0]\n",
        "    plt.figure(figsize=(10,5))\n",
        "    df_grouped = df.groupby(x_var)[y_var].value_counts(normalize=True).unstack(y_var)*100\n",
        "    df_grouped.plot.barh(stacked=True)\n",
        "    plt.legend(\n",
        "        bbox_to_anchor=(1.05, 1),\n",
        "        loc=\"upper left\",\n",
        "        title=y_var)\n",
        "\n",
        "    plt.title(\"% of patients at the risk of CHD by: \"+i)\n",
        "    for ix, row in df_grouped.reset_index(drop=True).iterrows():\n",
        "        # print(ix, row)\n",
        "        cumulative = 0\n",
        "        for element in row:\n",
        "            if element > 0.1:\n",
        "                plt.text(\n",
        "                    cumulative + element / 2,\n",
        "                    ix,\n",
        "                    f\"{int(element)} %\",\n",
        "                    va=\"center\",\n",
        "                    ha=\"center\",\n",
        "                )\n",
        "            cumulative += element\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbFf2-_FphqN"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loh7H2nzphqN"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ouA3fa0phqN"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VECbqPI7phqN"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Seke61FWphqN"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW4_bGpfphqN"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIIx-8_IphqN"
      },
      "source": [
        "#### Chart - 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "outputs": [],
      "source": [
        "# Chart - 7 visualization code\n",
        "for col in continuous_var:\n",
        "    plt.figure(figsize=(10,5))\n",
        "    sns.distplot(df[col])\n",
        "    plt.axvline(df[col].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "    plt.axvline(df[col].median(), color='cyan', linestyle='dashed', linewidth=2)\n",
        "    plt.title(col+' distribution')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t27r6nlMphqO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv6ro40sphqO"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2jJGEOYphqO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po6ZPi4hphqO"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0JNsNcRphqO"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvSq8iUTphqO"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZR9WyysphqO"
      },
      "source": [
        "#### Chart - 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "outputs": [],
      "source": [
        "# Chart - 8 visualization code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj7wYXLtphqO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob8u6rCTphqO"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZrbJ2SmphqO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZtgC_hjphqO"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFu4xreNphqO"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey_0qi68phqO"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ55k-q6phqO"
      },
      "source": [
        "#### Chart - 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "outputs": [],
      "source": [
        "# Chart - 9 visualization code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCFgpxoyphqP"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVxDimi2phqP"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVtJsKN_phqQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngGi97qjphqQ"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lssrdh5qphqQ"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBpY5ekJphqQ"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      },
      "source": [
        "#### Chart - 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "outputs": [],
      "source": [
        "# Chart - 10 visualization code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M8mcRywphqQ"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8agQvks0phqQ"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgIPom80phqQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp13pnNzphqQ"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMzcOPDDphqR"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4Ka1PC2phqR"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-EpHcCOp1ci"
      },
      "source": [
        "#### Chart - 11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "outputs": [],
      "source": [
        "# Chart - 11 visualization code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_VqEhTip1ck"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vsMzt_np1ck"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zGJKyg5p1ck"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "druuKYZpp1ck"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3dbpmDWp1ck"
      },
      "source": [
        "#### Chart - 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "outputs": [],
      "source": [
        "# Chart - 12 visualization code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylSl6qgtp1ck"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2xqNkiQp1ck"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWILFDl5p1ck"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-lUsV2mp1ck"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7G43BXep1ck"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wwDJXsLp1cl"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag9LCva-p1cl"
      },
      "source": [
        "#### Chart - 13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "outputs": [],
      "source": [
        "# Chart - 13 visualization code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6MkPsBcp1cl"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V22bRsFWp1cl"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cELzS2fp1cl"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MPXvC8up1cl"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL8l1tdLp1cl"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC_X3p0fY2L0"
      },
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "outputs": [],
      "source": [
        "# Correlation Heatmap visualization code"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rDvYKiu8kCFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CudH1xnlkCFZ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title('Correlation Analysis')\n",
        "correlation=df[continuous_var].corr()\n",
        "sns.heatmap(abs(correlation),annot=True,cmap='coolwarm')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Range of systolic bp and diastolic bp\n",
        "\n",
        "print(df['systolic_bp'].min(),df['systolic_bp'].max())\n",
        "print(df['diastolic_bp'].min(),df['diastolic_bp'].max())\n"
      ],
      "metadata": {
        "id": "sa1muG0pkCFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5L-zReekCFa"
      },
      "outputs": [],
      "source": [
        "# Range of systolic bp and diastolic bp\n",
        "\n",
        "print(df['systolic_bp'].min(),df['systolic_bp'].max())\n",
        "print(df['diastolic_bp'].min(),df['diastolic_bp'].max())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVkjeHzvkCFb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNZb8eZJkCFb"
      },
      "outputs": [],
      "source": [
        "# Creating a new column pulse_pressure\n",
        "# and dropping systolic_bp and diastolic_bp\n",
        "\n",
        "df['pulse_pressure'] = df['systolic_bp']-df['diastolic_bp']\n",
        "df.drop('systolic_bp',axis=1,inplace=True)\n",
        "df.drop('diastolic_bp',axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XTW1KwlkCFc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QqWWczVkCFc"
      },
      "outputs": [],
      "source": [
        "# columns\n",
        "df.columns\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsSDh9JgkCFd"
      },
      "outputs": [],
      "source": [
        "# Updating the continuous_var list\n",
        "\n",
        "continuous_var.remove('systolic_bp')\n",
        "continuous_var.remove('diastolic_bp')\n",
        "continuous_var.append('pulse_pressure')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "To2RAdTdkCFe"
      },
      "outputs": [],
      "source": [
        "# Analyzing the distribution of pulse_pressure\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot(df['pulse_pressure'])\n",
        "plt.axvline(df['pulse_pressure'].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "plt.axvline(df['pulse_pressure'].median(), color='cyan', linestyle='dashed', linewidth=2)\n",
        "plt.title('Pulse Pressure Distribution')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3izM_dQNkCFe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relationship between pulse pressure with the dependent variable\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.catplot(x=dependent_var[0],y='pulse_pressure',data=df,kind='violin')\n",
        "plt.title('ten_year_chd vs pulse_pressure')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zDQyWlEckCFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BgNBKXULkCFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated correlations\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.title('Correlation Analysis')\n",
        "correlation = df[continuous_var].corr()\n",
        "sns.heatmap(abs(correlation), annot=True, cmap='coolwarm')\n"
      ],
      "metadata": {
        "id": "jNezbsAOkCFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZDsbatZkCFm"
      },
      "source": [
        "\n",
        "What all manipulations have you done and insights you found?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tapfmDENkCFn"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q29F0dvdveiT"
      },
      "source": [
        "#### Chart - 15 - Pair Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "outputs": [],
      "source": [
        "# Pair Plot visualization code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXh0U9oCveiU"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMmPjTByveiU"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22aHeOlLveiV"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPQ8RGwHveiV"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SrGgzUZVV0dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAhexpZi7q1e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sf0_tk9VOsD1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C6IXAnkcOt8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "0zABgQ6fOuwo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "LcXeDxriOuwo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Individuals aged 50 and above exhibit a heightened risk of TenYearCHD.\n",
        "\n",
        "2) The likelihood of developing CHD is greater for diabetic patients compared to non-diabetic individuals.\n",
        "\n",
        "3) Elevated total cholesterol levels correlate with an elevated CHD risk."
      ],
      "metadata": {
        "id": "uKngqH2BOuwp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "UdChIDchOuwp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "UVAvkErCOuwp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Individuals aged 50 and above exhibit a heightened risk of TenYearCHD.*\n",
        "\n",
        "Null hypothesis - Age has no effect on the risk of TenYearCHD.\n",
        "\n",
        "Alternative hypothesis - Patients over 50 years of age have a higher risk of TenYearCHD than those who are under 50 years of age."
      ],
      "metadata": {
        "id": "60bTAvRqOuwp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "HWgkz62wOuwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "import statsmodels.stats.proportion as smp\n",
        "\n",
        "# Separate the dataset into two groups based on age\n",
        "above_50 = df[df['age'] > 50]\n",
        "below_50 = df[df['age'] <= 50]\n",
        "\n",
        "# Calculate the proportion of patients with TenYearCHD in each group\n",
        "prop_above_50 = above_50['ten_year_chd'].mean()\n",
        "prop_below_50 = below_50['ten_year_chd'].mean()\n",
        "\n",
        "# Perform a one-tailed z-test to compare the proportions of the two groups\n",
        "z_score, p_val = smp.proportions_ztest([prop_above_50 * len(above_50), prop_below_50 * len(below_50)], [len(above_50), len(below_50)], alternative='larger')\n",
        "\n",
        "print('z_score=%.3f, p_val=%.3f' % (z_score, p_val))\n",
        "\n",
        "if p_val < 0.05:\n",
        "    print('Reject Null Hypothesis')\n",
        "else:\n",
        "    print('Accept Null Hypothesis')\n",
        "\n",
        "# Print the p-value\n",
        "print('p-value:', p_val)"
      ],
      "metadata": {
        "id": "h1D-0l3rOuwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test results suggest that the probability of the observed difference in TenYearCHD risk between patients above 50 years of age and those below 50 years of age occurring by chance is exceedingly low.\n",
        "\n",
        "As a result, we reject the null hypothesis and draw the conclusion that individuals aged above 50 face a significantly greater risk of TenYearCHD compared to those below 50 years of age."
      ],
      "metadata": {
        "id": "XqsKPLHZy6lf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "gibd51BWOuwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I employed a one-tailed Z-test to assess the disparities in TenYearCHD proportions between patients aged above and below 50 years."
      ],
      "metadata": {
        "id": "63uRrEi6Ouwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "obwZVbhWOuwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I opted for a one-tailed z-test to evaluate the proportions of the two groups because our focus is on determining whether the group aged above 50 years has a higher proportion of TenYearCHD patients compared to the group below 50 years. The z-test is suitable for such comparisons when dealing with large sample sizes."
      ],
      "metadata": {
        "id": "NS7aiMvrOuwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "eYYGfRzLOuwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "wIm1WUAPOuwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The likelihood of developing CHD is greater for diabetic patients compared to non-diabetic individuals.*\n",
        "\n",
        "Null hypothesis: There is no significant difference in the risk of developing CHD between diabetic and non-diabetic patients.\n",
        "\n",
        "Alternative hypothesis: Diabetic patients are at a higher risk of developing CHD than non-diabetic patients."
      ],
      "metadata": {
        "id": "F3TEnc8iOuwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "fpeHKQKYOuwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Separate the dataset into two groups based on diabetic status\n",
        "diabetic = df[df['diabetes'] == 1]\n",
        "non_diabetic = df[df['diabetes'] == 0]\n",
        "\n",
        "# Perform a two-sample t-test to compare the mean TenYearCHD rates of the two groups\n",
        "t_stat, p_val = stats.ttest_ind(diabetic['ten_year_chd'], non_diabetic['ten_year_chd'], equal_var=False)\n",
        "\n",
        "print('t_stat=%.3f, p_val=%.3f' % (t_stat, p_val))\n",
        "if p_val > 0.05:\n",
        "    print('Accept Null Hypothesis')\n",
        "else:\n",
        "    print('Reject Null Hypothesis')\n",
        "\n",
        "# Print the p-value\n",
        "print('p-value:', p_val)"
      ],
      "metadata": {
        "id": "w8E6j7UxOuwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The t-statistic quantifies the difference in means between diabetic and non-diabetic patients, adjusted by the standard error of this difference. Meanwhile, the p-value signifies the likelihood of encountering such a disparity in means purely by random chance, under the assumption of the null hypothesis.\n",
        "\n",
        "With a obtained p-value of 0.000, which is less than the significance level of 0.05, it indicates an extremely low likelihood of observing such a difference in means through random chance. Consequently, we reject the null hypothesis and deduce that diabetic patients face a heightened risk of developing CHD compared to non-diabetic patients."
      ],
      "metadata": {
        "id": "QSMexfzjzsac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "UeOjgDN3Ouwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I opted for two-sample t-test to calculate the p-value concerning the hypothesis that \"Diabetic patients face a greater risk of developing CHD compared to non-diabetic patients.\""
      ],
      "metadata": {
        "id": "9vt4TWKHOuws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "pp5XfKujOuws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice of a two-sample t-test stems from the need to compare the means of two distinct and independent groups, specifically diabetic and non-diabetic individuals, in relation to the binary outcome variable of CHD risk. This statistical test is well-suited for our analysis, as it enables the assessment of whether there exists a statistically significant disparity between the means of these two groups. Moreover, given the relatively substantial sample sizes in both groups, the t-test stands out as a robust and dependable method for this evaluation."
      ],
      "metadata": {
        "id": "6lOtztP5Ouws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "7XtsEs2AOuws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "oJCdTSn6Ouws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Elevated total cholesterol levels correlate with an elevated CHD risk.*\n",
        "\n",
        "Null Hypothesis - There is no difference in the mean total cholesterol levels between the two groups\n",
        "\n",
        "Alternate Hypothesis - There is a significant difference in the mean total cholesterol levels between the two groups."
      ],
      "metadata": {
        "id": "J-qd_7sDOuws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "HLJmHA9xOuwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Import the required statistical test module from scipy\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Separate the dataset into two groups based on CHD status\n",
        "chd = df[df['ten_year_chd'] == 1] # Patients with CHD\n",
        "no_chd = df[df['ten_year_chd'] == 0] # Patients without CHD\n",
        "\n",
        "# Perform a two-sample t-test to compare the mean total cholesterol levels of the two groups\n",
        "t_stat, p_val = stats.ttest_ind(chd['total_cholesterol'], no_chd['total_cholesterol'], equal_var=False)\n",
        "\n",
        "# Print the calculated t-statistic and p-value\n",
        "print('t_stat=%.3f, p_val=%.3f' % (t_stat, p_val))\n",
        "\n",
        "# Determine if the null hypothesis should be rejected based on the p-value\n",
        "if p_val < 0.05:\n",
        "    print('Reject the null hypothesis')\n",
        "else:\n",
        "    print('Fail to reject the null hypothesis')\n",
        "\n",
        "# Print the p-value\n",
        "print('p-value:', p_val)"
      ],
      "metadata": {
        "id": "xVbF3G4nOuwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The obtained p-value is remarkably minuscule (p_val=5.310852329016078e-07), significantly less than the conventional significance level of 0.05.\n",
        "\n",
        "Consequently, we have grounds to reject the null hypothesis, which posits no divergence in total cholesterol levels between the two groups (CHD and no CHD).\n",
        "\n",
        "This observation strongly implies that heightened total cholesterol levels are linked to an elevated risk of CHD.\n",
        "\n",
        "Further corroborating this conclusion is the t-statistic of 5.065, indicating a notable dissimilarity in means between the two groups."
      ],
      "metadata": {
        "id": "BiPojaaJ0VXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "jGDnwRt0Ouwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A two-sample t-test was performed to obtain the p-value. The t-test was used to compare the mean total cholesterol levels between two groups, one with CHD and the other without CHD, to determine if there is a significant difference between the two."
      ],
      "metadata": {
        "id": "rf6LLPLROuwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "d2SiN5Y6Ouwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To assess the hypothesis that \"Elevated total cholesterol levels are linked to an increased CHD risk,\" the most suitable statistical test to employ is the two-sample t-test. This choice arises from our need to compare the average total cholesterol levels in two distinct and independent groups: those afflicted with CHD and those without it. Given the binary nature of the outcome variable (CHD status), our objective is to determine whether there exists a substantial disparity in total cholesterol levels between these two groups. The two-sample t-test is a widely employed statistical method for comparing the means of two independent groups, operating under the assumptions of normal data distribution and potentially unequal variances between the groups."
      ],
      "metadata": {
        "id": "vWvPKo8HOuwt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLjJCtPM0KBk"
      },
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing*** *italicized text*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiyOF9F70UgQ"
      },
      "source": [
        "### 1. Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "outputs": [],
      "source": [
        "# Handling Missing Values & Missing Value Imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wuGOrhz0itI"
      },
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ixusLtI0pqI"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id1riN9m0vUs"
      },
      "source": [
        "### 2. Handling Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "outputs": [],
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Creating subplots with a 2x4 grid and a specific figure size (30x15)\n",
        "fig, axes = plt.subplots(2, 4, figsize=(30, 15))\n",
        "\n",
        "# Flatten the array of axes for easy iteration\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Iterating through the axes and columns\n",
        "for ax, col in zip(axes, continuous_var):\n",
        "    # Create a boxplot for the current column and assign it to the current axis\n",
        "    sns.boxplot(df[col], ax=ax)\n",
        "\n",
        "    # Setting the title of the current subplot with the column name (title-cased)\n",
        "    ax.set_title(col.title(), weight='bold')\n",
        "\n",
        "# Ensuring proper spacing and layout\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining a function to create a DataFrame of total outliers and percentage of outliers\n",
        "def outliers_df(df, continuous_features):\n",
        "    # Initializing an empty DataFrame to store the outlier information\n",
        "    outlier_df = pd.DataFrame(columns=['feature', 'lower_limit', 'upper_limit',\n",
        "                                       'IQR', 'total_outliers', 'percentage_outliers(%)'])\n",
        "\n",
        "    # Iterating through each continuous feature\n",
        "    for feature in continuous_features:\n",
        "        # Extracting the values of the current feature\n",
        "        values = df[feature]\n",
        "\n",
        "        # Calculating the quartiles (Q1, Q2, Q3)\n",
        "        q1, q2, q3 = values.quantile([0.25, 0.5, 0.75])\n",
        "\n",
        "        # Calculating the Interquartile Range (IQR)\n",
        "        iqr = q3 - q1\n",
        "\n",
        "        # Defining lower and upper limits for outliers\n",
        "        lower_limit = q1 - 1.5 * iqr\n",
        "        upper_limit = q3 + 1.5 * iqr\n",
        "\n",
        "        # Identifying outliers based on the limits\n",
        "        outliers = values[(values < lower_limit) | (values > upper_limit)]\n",
        "\n",
        "        # Calculating the total number of outliers and their percentage\n",
        "        total_outliers = len(outliers)\n",
        "        percentage_outliers = round(total_outliers * 100 / len(values), 2)\n",
        "\n",
        "        # Appending the outlier information to the outlier_df DataFrame\n",
        "        outlier_df = outlier_df.append({'feature': feature,\n",
        "                                        'lower_limit': lower_limit,\n",
        "                                        'upper_limit': upper_limit,\n",
        "                                        'IQR': iqr,\n",
        "                                        'total_outliers': total_outliers,\n",
        "                                        'percentage_outliers(%)': percentage_outliers},\n",
        "                                        ignore_index=True)\n",
        "\n",
        "    # Sorting the DataFrame by percentage of outliers in descending order\n",
        "    return outlier_df.sort_values(by=['percentage_outliers(%)'], ascending=False)\n"
      ],
      "metadata": {
        "id": "alSIxpX3XCsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outliers_df(df,continuous_var)"
      ],
      "metadata": {
        "id": "51DrxxMSXCl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# applying transformation for treating outlier\n",
        "df[continuous_var] = np.log(df[continuous_var] +1 )"
      ],
      "metadata": {
        "id": "Jm3zyFuTYJKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "578E2V7j08f6"
      },
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGZz5OrT1HH-"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89xtkJwZ18nB"
      },
      "source": [
        "### 3. Categorical Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "outputs": [],
      "source": [
        "# Encode your categorical columns\n",
        "df['sex'] = pd.get_dummies(df['sex'], drop_first=True)\n",
        "df['is_smoking'] = pd.get_dummies(df['is_smoking'], drop_first=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67NQN5KX2AMe"
      },
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDaue5h32n_G"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "A1BIyPLXR7Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6iYu0ygMR9Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      },
      "source": [
        "### 4. Feature Manipulation & Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C74aWNz2AliB"
      },
      "source": [
        "#### 1. Feature Manipulation"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JAw536QLSDNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DejudWSA-a0"
      },
      "source": [
        "#### 2. Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "outputs": [],
      "source": [
        "# Select your features wisely to avoid overfitting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "f,ax = plt.subplots(figsize=(12, 12))\n",
        "sns.heatmap(abs(round(df.corr(),3)), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XysXGagOSkoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEMng2IbBLp7"
      },
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      },
      "source": [
        "##### Which all features you found important and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGgaEstsBnaf"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "5ac_WvEUTpAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K2YdezymTpC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Making the final DataFrame\n",
        "final_df = df[['age', 'education', 'sex','cigs_per_day', 'bp_meds',\n",
        "               'prevalent_stroke', 'prevalent_hyp', 'diabetes', 'total_cholesterol',\n",
        "               'bmi', 'heart_rate', 'glucose', 'pulse_pressure', 'ten_year_chd']]"
      ],
      "metadata": {
        "id": "Hjs5b4ABTpFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YJ4z_32fUJcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for heatmap if anything remains to avoid multicollinearity\n",
        "plt.figure(figsize=(15,15))\n",
        "correlation = final_df.corr()\n",
        "sns.heatmap((correlation), annot=True, cmap=sns.color_palette(\"mako\", as_cmap=True))"
      ],
      "metadata": {
        "id": "mr83mrLuUOFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNVZ9zx19K6k"
      },
      "source": [
        "### 5. Data Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqoHp30x9hH9"
      },
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "outputs": [],
      "source": [
        "# Transform Your data\n",
        "plt.figure(figsize=(10, 5))\n",
        "print(\"Distribution before Transformation\")\n",
        "\n",
        "# Plotting the distribution of 'pulsePressure' feature\n",
        "sns.histplot(df['pulse_pressure'],kde = 'True',element='step')\n",
        "\n",
        "# Setting a title for the plot\n",
        "plt.title('Distribution of Pulse Pressure')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking whether feature is guassian or normal distributed\n",
        "#Q-Q plot\n",
        "stats.probplot(df['pulse_pressure'],dist='norm',plot=pylab)"
      ],
      "metadata": {
        "id": "XyWlWm6eY0dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating copies to check the distribution of the variable\n",
        "test_df1=final_df.copy()\n",
        "test_df2=final_df.copy()\n",
        "test_df3=final_df.copy()\n",
        "test_df4=final_df.copy()"
      ],
      "metadata": {
        "id": "whmNlZsTY0ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Logarithmic Transformation on the considered column\n",
        "test_df1['pulse_pressure']=np.log(test_df1['pulse_pressure']+1)\n",
        "\n",
        "# Checking the distribution of continous variable\n",
        "plt.figure(figsize=(10,5))\n",
        "print(\"After Applying Transformation\")\n",
        "sns.histplot(test_df1['pulse_pressure'],kde='True',element='step')\n",
        "plt.title('Distribution of pulsePressure')"
      ],
      "metadata": {
        "id": "-QSNf96rY0Vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q-Q plot for log transformation.\n",
        "stats.probplot(test_df1['pulse_pressure'],dist='norm',plot=pylab)"
      ],
      "metadata": {
        "id": "bLTeZ6eoY0Ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Reciprocal Transformation on the considered column\n",
        "test_df2['pulse_pressure']=1/(test_df2['pulse_pressure']+1)\n",
        "\n",
        "# Checking the distribution of continous variable\n",
        "plt.figure(figsize=(10,5))\n",
        "print(\"After Applying Transformation\")\n",
        "sns.histplot(test_df2['pulse_pressure'],kde='True',element='step')\n",
        "plt.title('Distribution of pulsePressure')"
      ],
      "metadata": {
        "id": "sQXQkm-TZqlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-Q plot for Reciprocal Transsformation\n",
        "stats.probplot(test_df2['pulse_pressure'],dist='norm',plot=pylab)"
      ],
      "metadata": {
        "id": "SvC_SiBzZqoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Square Root Transformation on the considered column\n",
        "test_df3['pulse_pressure']=(test_df3['pulse_pressure'])**(1/2)\n",
        "\n",
        "# Checking the distribution of continous variable\n",
        "plt.figure(figsize=(10,5))\n",
        "print(\"After Applying Transformation\")\n",
        "sns.histplot(test_df3['pulse_pressure'],kde='True',element='step')\n",
        "plt.title('Distribution of pulsePressure')"
      ],
      "metadata": {
        "id": "64dEf-wDZqq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-Q plot for Square Root Transformation\n",
        "stats.probplot(test_df3['pulse_pressure'],dist='norm',plot=pylab)"
      ],
      "metadata": {
        "id": "utxMYiEiZqts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Exponential Transformation on the considered column\n",
        "test_df4['pulse_pressure']=(test_df4['pulse_pressure'])**(1/1.2)\n",
        "\n",
        "# Checking the distribution of continous variable\n",
        "plt.figure(figsize=(10,5))\n",
        "print(\"After Applying Transformation\")\n",
        "sns.histplot(test_df4['pulse_pressure'],kde='True',element='step')\n",
        "plt.title('Distribution of pulsePressure')"
      ],
      "metadata": {
        "id": "A70l4ZtqZqwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-Q plot for Exponential Transformtion.\n",
        "stats.probplot(test_df4['pulse_pressure'],dist='norm',plot=pylab)"
      ],
      "metadata": {
        "id": "7X3oLGCgZqyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6dayhTJwfIoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "drjnoSv_fIkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying transformation on the considered column\n",
        "## Logarithmic transformation\n",
        "final_df['pulse_pressure']=np.log(final_df['pulse_pressure']+1)"
      ],
      "metadata": {
        "id": "aZ6A02hoZq1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PMPAPt0tfGzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XoYdZUnEfG2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7DTxVna7fHEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMDnDkt2B6du"
      },
      "source": [
        "### 6. Data Scaling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before scaling our data let's just seperate our dependent and indepedent variables"
      ],
      "metadata": {
        "id": "q-fj8goHfbbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=final_df.drop('ten_year_chd',axis=1)\n",
        "y=final_df[['ten_year_chd']]\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "75yxNbTNfeW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scaling yout data\n",
        "# creating object\n",
        "std_regressor=StandardScaler()\n",
        "\n",
        "#fit and transform\n",
        "x=std_regressor.fit_transform(x)"
      ],
      "metadata": {
        "id": "TL7fy0edfs1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiiVWRdJDDil"
      },
      "source": [
        "##### Which method have you used to scale you data and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "StandardScaler is a widely employed technique in machine learning to normalize data by centering it around a mean of 0 and ensuring a standard deviation of 1. This approach is commonly adopted as it retains the original distribution's shape and is well-suited for a broad range of machine learning algorithms, particularly those reliant on distance-based metrics. Moreover, StandardScaler proves valuable when dealing with data featuring substantial variations in feature scales, as it facilitates better comparability among these features."
      ],
      "metadata": {
        "id": "dTYmcFSYhAyb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UUpS68QDMuG"
      },
      "source": [
        "### 7. Dimesionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kexQrXU-DjzY"
      },
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      },
      "source": [
        "NO it's not needed.\n",
        "\n",
        "However, in this case of the cardiovascular risk prediction dataset, it is not necessary to perform dimensionality reduction. This is because the dataset has a relatively small number of features compared to the number of samples, which means that the risk of overfitting is low. Additionally, the dataset is relatively small, so the training time for machine learning models would not be a significant issue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "outputs": [],
      "source": [
        "# DImensionality Reduction (If needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhH2vgX9EjGr"
      },
      "source": [
        "### 8. Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "outputs": [],
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n",
        "print(f'The Shape of x_train is:{x_train.shape}')\n",
        "print(f'The Shape of y_train is:{y_train.shape}')\n",
        "print(f'The Shape of x_test is:{x_test.shape}')\n",
        "print(f'The Shape of y_test is:{y_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjKvONjwE8ra"
      },
      "source": [
        "##### What data splitting ratio have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      },
      "source": [
        "In order to enhance model generalization and avoid overfitting, we divided the data into two sets: 80% for training and 20% for testing. We accomplished this by employing the \"train_test_split\" function from the scikit-learn library, a widely used method that separates data into distinct training and testing subsets. This approach allows us to train the model on one portion of the data while evaluating its performance on an independent dataset, promoting a more reliable assessment of the model's capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1XJ9OREExlT"
      },
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.ten_year_chd.value_counts())"
      ],
      "metadata": {
        "id": "_MiMqwkYitgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate value counts of 'TenYearCHD' column\n",
        "counts =df['ten_year_chd'].value_counts()\n",
        "\n",
        "# set labels and colors for the pie chart\n",
        "labels = ['NO','YES']\n",
        "colors = ['lightcoral','lightgreen']\n",
        "\n",
        "# create pie chart\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.pie(counts, labels=labels, colors=colors, autopct= \"%1.1f%%\",\n",
        "        startangle=90, shadow=True, explode=[0,0])\n",
        "\n",
        "\n",
        "#add title to the chart\n",
        "plt.title('TenYearCHD Distribution', fontsize=16)\n",
        "\n",
        "#display the chart\n",
        "plt.show"
      ],
      "metadata": {
        "id": "D5gKY3nki4Zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFOzZv6IFROw"
      },
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeKDIv7pFgcC"
      },
      "source": [
        "YES The pie chart provides a clear illustration of the data's class distribution concerning the 10-year risk of coronary heart disease (CHD). Notably, the data exhibits a substantial imbalance, with 84.9% of the total population (equivalent to 2,879 individuals) having no CHD risk, while only 15.1% (511 individuals) are considered at risk.\n",
        "\n",
        "This pronounced class imbalance can introduce bias into predictive models and potentially compromise their performance. To address this issue and enhance the accuracy and robustness of our models, it is imperative to employ appropriate techniques such as undersampling or oversampling to balance the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "outputs": [],
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "#initialize SMOTE\n",
        "smote=SMOTE(random_state=42)\n",
        "\n",
        "#fit and apply Smote to data\n",
        "x_resampled,y_resampled=smote.fit_resample(x,y)\n",
        "\n",
        "#print the orignal and resampled dataset shape\n",
        "print('Original dataset shape:', df.shape)\n",
        "print('Resampled dataset shape:', x_resampled.shape)\n",
        "\n",
        "# Count the number of samples in each class in the resampled dataset\n",
        "print('Class distribution in the resampled dataset:', y_resampled.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x_resampled, y_resampled,test_size=0.2,random_state=42)\n",
        "print(f'The shape of x_train is: {x_train.shape}')\n",
        "print(f'The shape of y_train is: {y_train.shape}')\n",
        "print(f'The shape of x_test is: {x_test.shape}')\n",
        "print(f'The shape of y_test is: {y_test.shape}')"
      ],
      "metadata": {
        "id": "XYxNXbWdm1iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIqpNgepFxVj"
      },
      "source": [
        "\n",
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbet1HwdGDTz"
      },
      "source": [
        "I applied the Synthetic Minority Over-sampling Technique (SMOTE) to address the imbalanced dataset. SMOTE is an oversampling method that creates synthetic samples for the minority class by interpolating new instances between existing ones. This approach is instrumental in rectifying class imbalance issues, as it mitigates the bias towards the majority class commonly observed in imbalanced datasets. By doing so, SMOTE contributes to enhancing the performance of machine learning models when dealing with such datasets."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iq8Ra5CdHLzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B64xBnOeHO0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "nl8pb8P0HPJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets Create a funtion that will give the evaluation metric and plot the results."
      ],
      "metadata": {
        "id": "w2Z-k6VkjWdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_metrics(y_train, y_test, train_preds, test_preds):\n",
        "    # Calculate and print training and testing accuracy\n",
        "    train_accuracy = accuracy_score(y_train, train_preds)\n",
        "    test_accuracy = accuracy_score(y_test, test_preds)\n",
        "\n",
        "    # Calculate and print training and testing precision\n",
        "    train_precision = precision_score(y_train, train_preds)\n",
        "    test_precision = precision_score(y_test, test_preds)\n",
        "\n",
        "    # Calculate and print training and testing recall\n",
        "    train_recall = recall_score(y_train, train_preds)\n",
        "    test_recall = recall_score(y_test, test_preds)\n",
        "\n",
        "    # Calculate and print training and testing ROC AUC scores\n",
        "    train_roc_auc = roc_auc_score(y_train, train_preds)\n",
        "    test_roc_auc = roc_auc_score(y_test, test_preds)\n",
        "\n",
        "    # Display the computed metrics\n",
        "    print(f\"{'Train Accuracy':<20}{train_accuracy:.4f}\")\n",
        "    print(f\"{'Test Accuracy':<20}{test_accuracy:.4f}\")\n",
        "    print(f\"{'Train Precision':<20}{train_precision:.4f}\")\n",
        "    print(f\"{'Test Precision':<20}{test_precision:.4f}\")\n",
        "    print(f\"{'Train Recall':<20}{train_recall:.4f}\")\n",
        "    print(f\"{'Test Recall':<20}{test_recall:.4f}\")\n",
        "    print(f\"{'Train ROC AUC':<20}{train_roc_auc:.4f}\")\n",
        "    print(f\"{'Test ROC AUC':<20}{test_roc_auc:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Generate and display confusion matrices for training and testing data\n",
        "    train_confusion_matrix = confusion_matrix(y_train, train_preds)\n",
        "    test_confusion_matrix = confusion_matrix(y_test, test_preds)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    labels = ['0', '1']\n",
        "\n",
        "    # Plot the training confusion matrix\n",
        "    sns.heatmap(train_confusion_matrix, annot=True, cmap='Purples', ax=axes[0], fmt=\"d\", xticklabels=labels, yticklabels=labels)\n",
        "    axes[0].set_xlabel('Predicted labels')\n",
        "    axes[0].set_ylabel('True labels')\n",
        "    axes[0].set_title('Train Confusion Matrix')\n",
        "\n",
        "    # Plot the testing confusion matrix\n",
        "    sns.heatmap(test_confusion_matrix, annot=True, cmap='YlOrRd', ax=axes[1], fmt=\"d\", xticklabels=labels, yticklabels=labels)\n",
        "    axes[1].set_xlabel('Predicted labels')\n",
        "    axes[1].set_ylabel('True labels')\n",
        "    axes[1].set_title('Test Confusion Matrix')\n",
        "\n",
        "    # Show the plots\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "7X0IDXqyjgvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "q6ywEhkiHPJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***LOGISTIC REGRESSION***"
      ],
      "metadata": {
        "id": "dXfB9yMVnX6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "logistic_classifier= LogisticRegression()\n",
        "# Fit the Algorithm\n",
        "logistic_classifier.fit(x_train,y_train)\n",
        "# Predict on the model\n",
        "y_train_logistic_pred= logistic_classifier.predict(x_train)\n",
        "y_test_logistic_pred= logistic_classifier.predict(x_test)"
      ],
      "metadata": {
        "id": "iofrZDoRHPJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "1el3JiNLHPJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_metrics(y_train, y_test, y_train_logistic_pred, y_test_logistic_pred)"
      ],
      "metadata": {
        "id": "j-ihyWPsHPJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We began our analysis by employing a fundamental and straightforward binary classification model, Logistic Regression. Given the critical nature of our task, which is to predict patients at higher risk of infection in the future, our primary focus was on maximizing recall. This emphasis on recall aimed to minimize false negatives, which in our case stood at 173.\n",
        "\n",
        "Upon implementing this model, we achieved a recall of 0.69 for both the training and testing sets, accompanied by an AUC-ROC score of 68% and an accuracy of 67%.\n",
        "\n",
        "While these results are promising, we acknowledge the paramount importance of healthcare analysis and the need to strive for even better outcomes. Therefore, we explored and compared various models, assessing multiple metrics to ensure that we are making every effort to save as many patients as possible."
      ],
      "metadata": {
        "id": "hiai9xvSkuAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "ajnpRr1bHPJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "logistic_regression = LogisticRegression()\n",
        "# set up the parameter grid for hyperparameter tuning\n",
        "param_grid = {'penalty': ['l1', 'l2'],\n",
        "              'C': [0.1, 1.0, 10.0],\n",
        "              'solver': ['liblinear', 'saga']}\n",
        "# Fit the Algorithm\n",
        "grid_search = GridSearchCV(logistic_regression, param_grid, cv=5)\n",
        "grid_search.fit(x_train, y_train)\n",
        "# get the best hyperparameters and print them\n",
        "best_params = grid_search.best_params_\n",
        "print('Best hyperparameters:', best_params)\n",
        "# use the best hyperparameters to fit the model and make predictions\n",
        "logistic_regression_best = LogisticRegression(**best_params)\n",
        "# perform cross-validation on the model with the best hyperparameters\n",
        "cv_scores = cross_val_score(logistic_regression_best, x_train, y_train, cv=5)\n",
        "# fit the final model using all the training data and the best hyperparameters\n",
        "logistic_regression_best.fit(x_train, y_train)\n",
        "y_train_logistic_pred_cv = logistic_regression_best.predict(x_train)\n",
        "y_test_logistic_pred_cv  = logistic_regression_best.predict(x_test)\n",
        "y_score_logistic_pred_cv = logistic_regression_best.predict_proba(x_test)[:, 1]"
      ],
      "metadata": {
        "id": "4XPVFMNRHPJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "Ar-VNukwHPJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV proves to be a potent method for fine-tuning the hyperparameters of machine learning models. It meticulously explores all conceivable combinations of hyperparameters and their respective values, systematically evaluating them based on performance metrics. This exhaustive search process enables GridSearchCV to identify the optimal hyperparameter configuration, ultimately enhancing model accuracy and overall performance."
      ],
      "metadata": {
        "id": "e_iCY8AIHPJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "SnEFS2_AHPJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We enhanced our machine learning model's performance by employing GridSearchCV to meticulously explore hyperparameter combinations, seeking to identify the optimal configuration. Despite our efforts, we did not observe any substantial improvement in results.\n",
        "\n",
        "Our current results indicate a test accuracy of 67.97%, with test precision and test recall at 66.55% and 69.27%, respectively. Furthermore, the area under the curve (ROC AUC) stands at 0.68, which falls short of our expectations.\n",
        "\n",
        "To address this, we have decided to explore alternative models such as Random Forest and XGBoost. These models are expected to offer improved accuracy and AUC scores, thereby advancing the quality of our predictions."
      ],
      "metadata": {
        "id": "QjiQ4AbGHPJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "1xpHk3QRHPJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***RANDOM FOREST CLASSIFIER***"
      ],
      "metadata": {
        "id": "j68yJq8Snhic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While we considered using a Decision Tree as our foundational model, we opted for an ensemble technique known as Random Forest to attain significantly improved results. Random Forest harnesses the power of multiple decision trees, each trained on different subsets of the data. This ensemble approach effectively minimizes errors and enhances predictive accuracy by aggregating diverse tree predictions."
      ],
      "metadata": {
        "id": "pz7cG3gfwuxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2  Implementation\n",
        "random_forest = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=2, min_samples_leaf=1)\n",
        "\n",
        "# Fit the Algorithm\n",
        "random_forest.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_train_rf_pred = random_forest.predict(x_train)\n",
        "y_test_rf_pred = random_forest.predict(x_test)"
      ],
      "metadata": {
        "id": "wtIM30ognOrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zQMLKAcQHPJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_metrics(y_train, y_test, y_train_rf_pred, y_test_rf_pred)"
      ],
      "metadata": {
        "id": "klkt7EgUHPJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "SZ436KkeHPJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "random_forest = RandomForestClassifier()\n",
        "param_grid = {'n_estimators': [100, 200, 300],\n",
        "              'max_depth': [5, 10, 15, None],\n",
        "              'min_samples_split': [2, 5, 10],\n",
        "              'min_samples_leaf': [1, 2, 4]}\n",
        "# Fit the Algorithm\n",
        "grid_search = GridSearchCV(random_forest, param_grid, cv=5)\n",
        "grid_search.fit(x_train, y_train)\n",
        "# get the best hyperparameters and print them\n",
        "best_params = grid_search.best_params_\n",
        "print('Best hyperparameters:', best_params)\n",
        "# use the best hyperparameters to fit the model to the training data\n",
        "random_forest_best = RandomForestClassifier(**best_params)\n",
        "random_forest_best.fit(x_train, y_train)\n",
        "# Predict on the model\n",
        "y_train_rf_pred_gs = random_forest_best.predict(x_train)\n",
        "y_test_rf_pred_gs  = random_forest_best.predict(x_test)\n",
        "y_score_rf_pred_gs = random_forest_best.predict_proba(x_test)[:, 1]"
      ],
      "metadata": {
        "id": "yZAvOSVrHPJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_metrics(y_train, y_test, y_train_rf_pred_gs, y_test_rf_pred_gs)"
      ],
      "metadata": {
        "id": "GmKFGMt3w3AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "sm-o4f8IHPJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV proves to be a potent method for fine-tuning the hyperparameters of machine learning models. It meticulously explores all conceivable combinations of hyperparameters and their respective values, systematically evaluating them based on performance metrics. This exhaustive search process enables GridSearchCV to identify the optimal hyperparameter configuration, ultimately enhancing model accuracy and overall performance."
      ],
      "metadata": {
        "id": "ItFk01ADHPJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "8Mka3sRsHPJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Through the diligent use of GridSearchCV, we significantly enhanced the performance of our machine learning model by pinpointing the optimal hyperparameters. GridSearchCV systematically evaluates every conceivable combination of hyperparameters, ultimately selecting values that maximize model performance and yield the most precise outcomes.\n",
        "\n",
        "Following hyperparameter tuning, we arrived at the best parameter values, specifically 'min_samples_leaf': 1, 'min_samples_split': 2, and 'n_estimators': 200. Interestingly, while we achieved a perfect training accuracy of 100%, our test accuracy also experienced substantial improvement, rising from 83.07% to an impressive 88.89%.\n",
        "\n",
        "Furthermore, our efforts led to a substantial improvement in the ROC AUC score, escalating it from 0.8311 to a commendable 0.8890. These results underscore the effectiveness of hyperparameter tuning in refining our model's predictive capabilities."
      ],
      "metadata": {
        "id": "HLPSqbVlHPJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "AP6uTm1DHPKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assessing the performance of a machine learning model is essential for gauging the accuracy of its predictions. To achieve this, we utilized a range of metrics such as Accuracy, Precision, Recall, and ROC AUC score. These metrics allowed us to closely examine how well the model's predictions aligned with the actual values.\n",
        "\n",
        "The outcomes of our evaluation revealed that the model excelled in predicting Ten Year Coronary Heart Disease (CHD) with an impressive accuracy of approximately 88.89%. This level of accuracy holds considerable significance, especially given the direct business implications associated with the dependent variable, TenYearCHD."
      ],
      "metadata": {
        "id": "rJOOCA_FHPKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "gjPt9TxnHPKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***XGBoost CLASSIFIER***"
      ],
      "metadata": {
        "id": "UUZQxF20xl3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "xgb = XGBClassifier()\n",
        "# Fit the Algorithm\n",
        "xgb.fit(x_train, y_train)\n",
        "# Predict on the model\n",
        "y_train_xgb_pred = xgb.predict(x_train)\n",
        "y_test_xgb_pred = xgb.predict(x_test)"
      ],
      "metadata": {
        "id": "YUSdvuOwHPKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "enhp9Y_aHPKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost, short for \"Extreme Gradient Boosting,\" is a powerful machine learning algorithm renowned for its exceptional predictive performance. It belongs to the ensemble learning family and works by combining the predictions of multiple weak learners (typically decision trees) to create a robust and accurate model. XGBoost excels in various tasks, such as classification and regression, by optimizing a loss function and employing a gradient boosting framework. Its distinguishing features include the ability to handle missing data, automatic handling of feature selection, and an efficient implementation that makes it computationally efficient and highly competitive in machine learning competitions. XGBoost's adaptability, scalability, and ability to capture complex relationships in data have made it a go-to choice for many data scientists and machine learning practitioners."
      ],
      "metadata": {
        "id": "dlH2b0Z1yWLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "model_metrics(y_train, y_test, y_train_xgb_pred, y_test_xgb_pred)"
      ],
      "metadata": {
        "id": "fgbpnILxHPKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "3AkOy9GDHPKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# set up the parameter grid for hyperparameter tuning\n",
        "param_grid = {'max_depth': [3, 5, 7],\n",
        "              'learning_rate': [0.01, 0.1, 0.3],\n",
        "              'n_estimators': [50, 100, 200]}\n",
        "# Fit the Algorithm\n",
        "grid_search = GridSearchCV(xgb, param_grid, cv=5, n_jobs=-1)\n",
        "grid_search.fit(x_train, y_train)\n",
        "# print the best hyperparameters\n",
        "print('Best hyperparameters:', grid_search.best_params_)\n",
        "# Predict on the model\n",
        "best_estimator = grid_search.best_estimator_\n",
        "y_train_xgb_pred_gs = best_estimator.predict(x_train)\n",
        "y_test_xgb_pred_gs  = best_estimator.predict(x_test)\n",
        "y_score_xgb_pred_gs = best_estimator.predict_proba(x_test)[:, 1]"
      ],
      "metadata": {
        "id": "aioYBKWjHPKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_metrics(y_train, y_test, y_train_xgb_pred_gs, y_test_xgb_pred_gs)"
      ],
      "metadata": {
        "id": "sBQrLjBiyEcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "4xhKyZEPHPKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV proves to be a potent method for fine-tuning the hyperparameters of machine learning models. It meticulously explores all conceivable combinations of hyperparameters and their respective values, systematically evaluating them based on performance metrics. This exhaustive search process enables GridSearchCV to identify the optimal hyperparameter configuration, ultimately enhancing model accuracy and overall performance."
      ],
      "metadata": {
        "id": "utRjxHnqHPKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "SznDc7nvHPKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "USing GridSearchCV to meticulously fine-tune our machine learning model's performance. This exhaustive process involved systematically evaluating all conceivable hyperparameter combinations to pinpoint the optimal values, resulting in heightened model accuracy and enhanced overall performance.\n",
        "\n",
        "Following hyperparameter tuning, we identified the best parameter values, specifically 'learning_rate': 0.1, 'max_depth': 7, and 'n_estimators': 200. The impact of these adjustments was substantial, as our model's accuracy surged significantly from 82.55% to an impressive 89.67%. Moreover, there were noteworthy improvements in Precision and Recall metrics, registering at 92.69% and 85.61%, respectively. Additionally, the ROC AUC score saw a substantial boost, reaching an impressive 0.8958, which is indicative of the model's strong predictive capabilities."
      ],
      "metadata": {
        "id": "ukvgD1ZdHPKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4"
      ],
      "metadata": {
        "id": "G_qdTDiI0L4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***K-Nearest Neighbors (KNN)***"
      ],
      "metadata": {
        "id": "4AXsxTWv3l22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 Implementation\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Fit the Algorithm\n",
        "knn.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_train_knn_pred = knn.predict(x_train)\n",
        "y_test_knn_pred = knn.predict(x_test)"
      ],
      "metadata": {
        "id": "Y_5oMRNS0hyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "EKC3U3Jc0RvD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Nearest Neighbors (KNN) is a simple yet effective machine learning algorithm used for both classification and regression tasks. It operates on the principle of proximity, where it assigns a data point's class or value based on the majority class or average of its k nearest neighbors in the feature space. KNN is intuitive, non-parametric, and adaptable to different data distributions, making it a versatile choice for various data-driven tasks. However, it can be sensitive to the choice of the number of neighbors (k) and requires efficient distance calculations for large datasets."
      ],
      "metadata": {
        "id": "oQ4pUw1Y1rKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_metrics(y_train, y_test, y_train_knn_pred, y_test_knn_pred)"
      ],
      "metadata": {
        "id": "zAuT1ZOn02LO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "lE59xRxO0Xm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4  Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# set up the parameter grid for hyperparameter tuning\n",
        "param_grid = {'n_neighbors': [3, 5, 7],\n",
        "              'weights': ['uniform', 'distance']}\n",
        "# Fit the Algorithm\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5)\n",
        "grid_search.fit(x_train, y_train)\n",
        "# get the best hyperparameters and print them\n",
        "best_params = grid_search.best_params_\n",
        "print('Best hyperparameters:', best_params)\n",
        "# train the classifier with the best hyperparameters on the full training set\n",
        "knn_best = KNeighborsClassifier(**best_params)\n",
        "knn_best.fit(x_train, y_train)\n",
        "# Predict on the model\n",
        "y_test_knn_pred_gs  = knn_best.predict(x_test)\n",
        "y_train_knn_pred_gs = knn_best.predict(x_train)\n",
        "y_score_knn_pred_gs = knn_best.predict_proba(x_test)[:, 1]"
      ],
      "metadata": {
        "id": "WLyu-JPt1lYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_metrics(y_train, y_test, y_train_knn_pred_gs, y_test_knn_pred_gs)"
      ],
      "metadata": {
        "id": "STvvZkMH2Y9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "fdFU0GpqfQcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV proves to be a potent method for fine-tuning the hyperparameters of machine learning models. It meticulously explores all conceivable combinations of hyperparameters and their respective values, systematically evaluating them based on performance metrics. This exhaustive search process enables GridSearchCV to identify the optimal hyperparameter configuration, ultimately enhancing model accuracy and overall performance."
      ],
      "metadata": {
        "id": "bZtmRfC1YoHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "03Zobf3WfY3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Through the meticulous utilization of GridSearchCV, we successfully fine-tuned our machine learning model by systematically exploring and selecting the best hyperparameters among all possible combinations. This rigorous optimization process led to a noteworthy enhancement in our model's performance.\n",
        "\n",
        "In the case of the K-Nearest Neighbors (KNN) model, we observed improved accuracy, which increased from 78.56% to 82.12%. Additionally, the model exhibited a Precision of 74.02% and an impressive Recall of 97.69%. The ROC AUC score, although higher after hyperparameter tuning, stood at 0.8246, which is slightly lower compared to the performance of the previous model."
      ],
      "metadata": {
        "id": "MfnTTEM0lVW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 5"
      ],
      "metadata": {
        "id": "Wbpundqk0NTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Support Vector Machine Classifier (SVC)***"
      ],
      "metadata": {
        "id": "dUGCVjIY3r54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 5 Implementation\n",
        "svc = SVC(kernel='rbf', C=1, gamma='scale')\n",
        "\n",
        "# Fit the Algorithm\n",
        "svc.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_train_svc_pred = svc.predict(x_train)\n",
        "y_test_svc_pred = svc.predict(x_test)"
      ],
      "metadata": {
        "id": "pHj8ZFfz1B-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TOz_bgua0S9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Support Vector Machine Classifier (SVC) is a powerful machine learning algorithm used for classification tasks. It works by finding the optimal hyperplane that best separates different classes in the data while maximizing the margin between them. SVC is particularly effective in handling complex data with non-linear boundaries, thanks to kernel functions that transform data into higher-dimensional spaces. It excels in scenarios where data separation isn't straightforward and aims to find the decision boundary that minimizes classification errors, making it a versatile choice for various classification problems."
      ],
      "metadata": {
        "id": "THhDnrSZ2qev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_metrics(y_train, y_test, y_train_svc_pred, y_test_svc_pred)"
      ],
      "metadata": {
        "id": "4U7QNWeZ1ZfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "NjYv8fTA0Z6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 5  Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "svc = SVC(probability=True)\n",
        "# set up the parameter grid for hyperparameter tuning\n",
        "param_grid = {'C': [0.1, 1, 10],\n",
        "              'kernel': ['linear', 'rbf'],\n",
        "              'gamma': ['scale', 'auto']}\n",
        "# perform a grid search with 5-fold cross-validation to find the best hyperparameters\n",
        "grid_search = GridSearchCV(svc, param_grid, cv=5)\n",
        "grid_search.fit(x_train, y_train)\n",
        "# get the best hyperparameters and print them\n",
        "best_params = grid_search.best_params_\n",
        "print('Best hyperparameters:', best_params)\n",
        "# train the classifier with the best hyperparameters on the full training set\n",
        "svc_best = SVC(**best_params, probability=True)\n",
        "svc_best.fit(x_train, y_train)\n",
        "# Predict on the model\n",
        "y_test_svc_pred_gs = svc_best.predict(x_test)\n",
        "y_train_svc_pred_gs = svc_best.predict(x_train)\n",
        "y_score_svc_pred_gs = svc_best.predict_proba(x_test)[:, 1]"
      ],
      "metadata": {
        "id": "sW8EA8KT2pbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_metrics(y_train, y_test, y_train_svc_pred_gs, y_test_svc_pred_gs)"
      ],
      "metadata": {
        "id": "wCIZSspf3Pmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "uoyfmPOrfNCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV proves to be a potent method for fine-tuning the hyperparameters of machine learning models. It meticulously explores all conceivable combinations of hyperparameters and their respective values, systematically evaluating them based on performance metrics. This exhaustive search process enables GridSearchCV to identify the optimal hyperparameter configuration, ultimately enhancing model accuracy and overall performance."
      ],
      "metadata": {
        "id": "2KtlQ-hLY6H7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "VMbsGEjyfXHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To enhance the performance of our machine learning model, we harnessed the capabilities of GridSearchCV, systematically exploring the full spectrum of hyperparameter combinations to identify optimal values. This meticulous optimization process was aimed at elevating model performance and achieving the utmost accuracy in our predictions.\n",
        "\n",
        "Following the hyperparameter tuning, we observed a modest yet notable improvement in our model's effectiveness. Accuracy advanced from 70.14% to 76.74%, precision saw an improvement from 68.65% to 73.30%, and recall exhibited a noteworthy increase from 71.58% to an impressive 82.42%. Additionally, our model achieved an AUC ROC score of 76.86%, indicative of its enhanced predictive capabilities."
      ],
      "metadata": {
        "id": "Q0NPj3HJsntC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 6"
      ],
      "metadata": {
        "id": "9y2RztM7fbvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Naive Bayes Classifier***"
      ],
      "metadata": {
        "id": "kCnDDGuf34Cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 6 Implementation\n",
        "# create an instance of the Gaussian Naive Bayes classifier\n",
        "nb = GaussianNB()\n",
        "\n",
        "# Fit the Algorithm\n",
        "nb.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_train_nb_pred = nb.predict(x_train)\n",
        "y_test_nb_pred = nb.predict(x_test)"
      ],
      "metadata": {
        "id": "BWbTEyVet6TV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "vZw3iLbJfhdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_metrics(y_train, y_test, y_train_nb_pred, y_test_nb_pred)"
      ],
      "metadata": {
        "id": "lwGrUssYuIpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "ry53IenuflGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 6  Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import cross_val_score\n",
        "# create an instance of the Gaussian Naive Bayes classifier\n",
        "nb = GaussianNB()\n",
        "# set up the parameter grid for hyperparameter tuning\n",
        "param_grid = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}\n",
        "# perform a grid search with cross-validation to find the best hyperparameters\n",
        "grid_search = GridSearchCV(nb, param_grid, cv=5)\n",
        "grid_search.fit(x_train, y_train)\n",
        "# get the best hyperparameters and print them\n",
        "best_params = grid_search.best_params_\n",
        "print('Best hyperparameters:', best_params)\n",
        "# create a new instance of the classifier using the best hyperparameters\n",
        "nb_best = GaussianNB(**best_params)\n",
        "# evaluate the classifier using cross-validation\n",
        "scores = cross_val_score(nb_best, x_train, y_train, cv=5)\n",
        "# print the cross-validation scores\n",
        "print('Cross-validation scores:', scores)\n",
        "# train the classifier on the entire training set using the best hyperparameters\n",
        "nb_best.fit(x_train, y_train)\n",
        "# make predictions on the training and test sets\n",
        "y_train_nb_pred_gs = nb_best.predict(x_train)\n",
        "y_test_nb_pred_gs = nb_best.predict(x_test)\n",
        "y_score_nb_pred_gs = nb_best.predict_proba(x_test)[:, 1]"
      ],
      "metadata": {
        "id": "Wi7CjYkfujtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_metrics(y_train, y_test, y_train_nb_pred_gs, y_test_nb_pred_gs)"
      ],
      "metadata": {
        "id": "Z-Bl-POcumBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "jKHqZWwgfObS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV proves to be a potent method for fine-tuning the hyperparameters of machine learning models. It meticulously explores all conceivable combinations of hyperparameters and their respective values, systematically evaluating them based on performance metrics. This exhaustive search process enables GridSearchCV to identify the optimal hyperparameter configuration, ultimately enhancing model accuracy and overall performance."
      ],
      "metadata": {
        "id": "rDbzTbOKu_Js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "q8Pk3O5kfWRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No Improvemets"
      ],
      "metadata": {
        "id": "gP4gSe3R4Kx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "80cX8n75HPKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of predicting TenYearCHD, a classification problem involves forecasting an outcome variable based on one or more predictor variables. Four key evaluation metrics are utilized:\n",
        "\n",
        "* **Accuracy:** This widely used metric gauges a classification model's effectiveness by assessing the percentage of correctly classified instances among all instances. A higher accuracy score signifies the model's superior ability to accurately predict the correct class for each instance.\n",
        "\n",
        "* **Precision:** Precision quantifies the ratio of true positive predictions to all positive predictions made by the model. It is computed by dividing true positives by the sum of true positives and false positives. A higher precision score signifies the model's capacity to minimize false positives, which is particularly critical in scenarios where such errors entail significant costs.\n",
        "\n",
        "* **Recall:** Also known as sensitivity or true positive rate, recall measures the proportion of true positive predictions among all instances actually belonging to the positive class. It is calculated by dividing true positives by the sum of true positives and false negatives. A higher recall score indicates the model's ability to reduce false negatives, which is crucial in situations where failing to identify positive cases carries substantial consequences.\n",
        "\n",
        "* **AUC ROC:** The Area Under the Receiver Operating Characteristic Curve (AUC ROC) assesses the performance of binary classification models. It evaluates the model's capacity to distinguish between positive and negative classes across various probability thresholds. AUC ROC scores range from 0 to 1, with 0.5 indicating random performance and 1 representing a flawless model. A higher AUC ROC score indicates the model's superior ability to differentiate between positive and negative classes accurately."
      ],
      "metadata": {
        "id": "jRjnr_QRHPKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "8zjU3qDCHPKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the classifiers\n",
        "classifiers = [ (\"Logistic Regression\", LogisticRegression()),\n",
        "                (\"Random Forest Classifier\", RandomForestClassifier()),\n",
        "                (\"XGB Classifier\", XGBClassifier()),\n",
        "                (\"KNN\", KNeighborsClassifier()),\n",
        "                (\"SVC\", SVC(probability=True)),\n",
        "                (\"NB Classifier\", GaussianNB())]\n",
        "\n",
        "# iterate through classifiers and plot ROC curves\n",
        "plt.figure(figsize=(8, 6))\n",
        "for name, classifier in classifiers:\n",
        "    classifier.fit(x_train, y_train)\n",
        "    y_score = classifier.predict_proba(x_test)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qt6GIqcPJopt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**After cross validation and hyperparameter tuning**"
      ],
      "metadata": {
        "id": "NSsq8H8aPpnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing metrics in order to make dataframe\n",
        "# (after cross validation and hyperparameter tuning)\n",
        "Model = [\"Logistic Regression\", \"Random Forest Classifier\", \"XGBoost\", \"KNN\", \"SVC\",\"NBClassifier\"]\n",
        "Y_SCORE = [y_score_logistic_pred_cv, y_score_rf_pred_gs, y_score_xgb_pred_gs,\n",
        "           y_score_knn_pred_gs, y_score_svc_pred_gs,y_score_nb_pred_gs]\n",
        "\n",
        "# Create dataframe from the lists\n",
        "data = {'MODEL': Model, 'Y_SCORE': Y_SCORE}\n",
        "Metric_df = pd.DataFrame(data)\n",
        "\n",
        "# plot the ROC curves for each model\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i, row in Metric_df.iterrows():\n",
        "    fpr, tpr, _ = roc_curve(y_test, row['Y_SCORE'])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"{row['MODEL']} (AUC = {roc_auc:.2f})\", alpha=0.8)\n",
        "plt.plot([0, 1], [0, 1], color='grey', linestyle='--', label='Random Guess')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BNqFvhSwIx0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing metrics in order to make dataframe of metrics\n",
        "# (after cross validation and hyperparameter tuning)\n",
        "Model          = [\"Logistic Regression\", \"Random Forest Classifier\", \"XGBoost\", \"KNN\", \"SVC\", \"NBClassifier\"]\n",
        "Test_Accuracy  = [0.6797,0.8889,0.8967,0.8212,0.7674,0.5885]\n",
        "Test_Precision = [0.6655,0.8796,0.9269,0.7402,0.7330,0.7330]\n",
        "Test_Recall    = [0.6927,0.8952,0.8561,0.9769,0.8242,0.2487]\n",
        "Test_ROC_AUC   = [0.6800,0.8890,0.8958,0.8246,0.7686,0.5810]\n",
        "# Create dataframe from the lists\n",
        "data = {'Model' : Model,\n",
        "        'Test_Accuracy'  : Test_Accuracy,\n",
        "        'Test_Precision' : Test_Precision,\n",
        "        'Test_Recall'    : Test_Recall,\n",
        "        'Test_ROC_AUC'   : Test_ROC_AUC}\n",
        "Metric_df = pd.DataFrame(data)\n",
        "\n",
        "# Printing dataframe\n",
        "Metric_df"
      ],
      "metadata": {
        "id": "n0U4QXFdKrmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After evaluating various models for the cardiovascular risk prediction project, it's evident that the Random Forest Classifier and XGBoost models stand out as the top choices for building the final prediction model. Both models exhibit high accuracy scores of 0.8889 and 0.8967, respectively, which is crucial for real-time prediction systems. Moreover, their precision and recall scores are also notably high, indicating their proficiency in correctly classifying both positive and negative cases.\n",
        "\n",
        "While the KNN model boasts a relatively high recall score, its accuracy and precision scores fall slightly behind those of the Random Forest Classifier and XGBoost models. Similarly, the SVC model's lower accuracy and ROC AUC score suggest it may not be the optimal choice for this specific classification problem.\n",
        "\n",
        "However, the XGBoost model edges out the Random Forest Classifier with slightly higher test accuracy and precision scores, making it a compelling choice for cardiovascular risk prediction. Additionally, its superior ROC AUC score signifies its enhanced ability to distinguish between positive and negative cases. **Therefore, based on these results, the XGBoost model emerges as the preferred classification model for predicting cardiovascular risk in this project.**"
      ],
      "metadata": {
        "id": "8wJ22v7x87zW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "wQwtQaheHPKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While tree-based algorithms may exhibit reduced interpretability, we can enhance interpretability through the use of tools like LIME and SHAP.\n",
        "\n",
        "Interpretability can be addressed from two perspectives:\n",
        "\n",
        "1) **Global Interpretability:** This entails comprehending the overarching relationship between features and prediction outcomes on a broader scale. For instance, linear regression is an example of a model that offers global interpretability as it provides insights into how each feature contributes to predictions across the entire dataset.\n",
        "\n",
        "2) **Local Interpretability:** This approach centers on understanding the individual influence of each feature on a specific prediction. Methods such as SHAP and LIME are illustrative of techniques that excel in providing local interpretability, as they shed light on the impact of features for a particular prediction instance."
      ],
      "metadata": {
        "id": "AVBsYhtvHPKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Global Explainability"
      ],
      "metadata": {
        "id": "487lOzpVlRTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the barplot to determine which feature is contributing the most\n",
        "features = final_df.columns\n",
        "importances = xgb.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "plt.figure(figsize=(8,10))\n",
        "plt.grid(zorder=0)\n",
        "plt.title('Feature Importances', fontsize=20)\n",
        "plt.barh(range(len(indices)), importances[indices], align='center')\n",
        "plt.yticks(range(len(indices)), features[indices])\n",
        "plt.xlabel('Relative Importance')"
      ],
      "metadata": {
        "id": "lNUkSQo8j6js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the percentage of feature importance\n",
        "feature_imp = pd.DataFrame(columns = ['Variable','Importance'])\n",
        "feature_imp.Variable = features[:-1]\n",
        "feature_imp.Importance = importances*100\n",
        "feature_imp.sort_values(by=\"Importance\",axis=0,ascending=False)"
      ],
      "metadata": {
        "id": "n5hMKxmQlTVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have considered XGboost as our final optimal model with very good accuracies but still this model is considered as black box model since we don't know actually what is happening inside the algorithm. In order to gain the trust of stakeholders we have to explain the model and under which conditions the model is predicting that particular result with a valid and senseful reason. So, in order to increase the explainability we have plotted the bar plot for decresing sequence of feature importance.\n",
        "\n",
        "From the above plot it is clear that for XGboost model \"age\" is contributing maximum i.e 30.57% in the final outcome, \"sex\" is contributing 27.13% and followed by \"pulse_pressure\", \"BP_Meds\"."
      ],
      "metadata": {
        "id": "EnF7Dnm9lmpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have achieved accuracy of 72% with the help of logistic regression but to attain more accuracy and eventually scale up your business we have done this with the help of Random forest, Xgboost but they are black box models(cant explain) so we use **MODEL EXPLAINABLITY** tool **SHAP**.  "
      ],
      "metadata": {
        "id": "4ufdngkOlqRB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Explanability using SHAP"
      ],
      "metadata": {
        "id": "ReTNFdkplswZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SHAP (**Shapley Additive exPlanations**) It is used to calculate the impact of each feature of the model on the final result."
      ],
      "metadata": {
        "id": "-ejWygiDl0GO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Here we are using TreeExplainer (for the analysis of decision trees)"
      ],
      "metadata": {
        "id": "rMkcAusWl2sM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "id": "_DpKijX8lXcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explaining decision tree with **ForcePlot**"
      ],
      "metadata": {
        "id": "FWnNDXHPl-B_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing our features into new variable\n",
        "feature = final_df.columns[:-1]"
      ],
      "metadata": {
        "id": "Fo18g9f5l5OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the values in the list\n",
        "feature"
      ],
      "metadata": {
        "id": "OoFf_GDxl_7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the first observation\n",
        "x_test[0:1]"
      ],
      "metadata": {
        "id": "FX3DOMtqmBJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "#create an explainer for a tree-based model, i.e., random forest (rf)\n",
        "explainer = shap.TreeExplainer(xgb)\n",
        "shap_values = explainer.shap_values(x_resampled[0:1])  #pass the first test sample"
      ],
      "metadata": {
        "id": "PsM8zjpNmC93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.initjs() #initialize the JavaScript visualization in the notebook environment\n",
        "shap.force_plot(explainer.expected_value, shap_values=shap_values[0], features = feature)"
      ],
      "metadata": {
        "id": "-Qxr26UrmKKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual value of  first observation\n",
        "y_resampled.head(1)"
      ],
      "metadata": {
        "id": "5peuS8bQmPUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Probability of each class (0 and 1)\n",
        "xgb.predict_proba(x_resampled[0:1])"
      ],
      "metadata": {
        "id": "i9UT9ctcmybn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicted value of  first observation\n",
        "xgb.predict(x_resampled[0:1])"
      ],
      "metadata": {
        "id": "wBHuUAM1m0MY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since, the predicted probability value of first observation of test set is more for class 1 (0.60) so every feature is cotributing and shifting the prediction towards higher side(+ve) from the base value."
      ],
      "metadata": {
        "id": "vlDc4kjmm-Ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pass the second test sample\n",
        "shap_values = explainer.shap_values(x_resampled[1:2])"
      ],
      "metadata": {
        "id": "_TEKdw-Lm9Mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize the JavaScript visualization in the notebook environment\n",
        "shap.initjs()\n",
        "shap.force_plot(explainer.expected_value, shap_values=shap_values[0], features = feature)"
      ],
      "metadata": {
        "id": "dYn9MGUUm20l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_resampled.head(2)"
      ],
      "metadata": {
        "id": "gvSeeuXtoLdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb.predict_proba(x_resampled[1:2])"
      ],
      "metadata": {
        "id": "N_3x14VWojUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb.predict(x_resampled[1:2])"
      ],
      "metadata": {
        "id": "7-_f9OUVolKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since, the predicted probability value of first observation of test set is more for class 0 (0.73) so every feature is cotributing and shifting the prediction towards lower side(-ve) from the base value."
      ],
      "metadata": {
        "id": "2lFQYUJGoqEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pass the third test sample\n",
        "shap_values = explainer.shap_values(x_resampled[2:3])"
      ],
      "metadata": {
        "id": "xLkrXW8TomYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize the JavaScript visualization in the notebook environment\n",
        "shap.initjs()\n",
        "shap.force_plot(explainer.expected_value, shap_values=shap_values[0], features = feature)"
      ],
      "metadata": {
        "id": "hN3mMzkqooEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_resampled.head(3)"
      ],
      "metadata": {
        "id": "rSG_Szw4owH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb.predict_proba(x_resampled[2:3])"
      ],
      "metadata": {
        "id": "A_598XRApI6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb.predict(x_resampled[2:3])"
      ],
      "metadata": {
        "id": "AIowHVzGpKuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since, the predicted probability value of third observation of test set is more for class 1 ( 0.763) so every feature is cotributing and shifting the prediction towards higher side(+ve) from the base value."
      ],
      "metadata": {
        "id": "pNlcl7RtpQBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "GDJAq9_9JV8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "8If5zWZKpTaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process"
      ],
      "metadata": {
        "id": "Gw8RxtUipV2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Save the File\n",
        "# Importing pickle module\n",
        "import pickle"
      ],
      "metadata": {
        "id": "RKeNaE4ZpMyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "filename='CHD_prediction.pkl'\n",
        "\n",
        "# serialize process (wb=write byte)\n",
        "pickle.dump(best_estimator,open(filename,'wb'))"
      ],
      "metadata": {
        "id": "G9aEKLxQpXnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "ZwdY_e7YpboK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "\n",
        "# unserialize process (rb=read byte)\n",
        "classification_model= pickle.load(open(filename,'rb'))\n",
        "\n",
        "# Predicting the unseen data(test set)\n",
        "classification_model.predict(x_resampled)"
      ],
      "metadata": {
        "id": "OMqZFj-SpZIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Our model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "kgykAv_SpgR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusions from Exploratory Data Analysis (EDA):**\n",
        "\n",
        "1. Age emerges as a pivotal factor influencing the risk of Coronary Heart Disease (CHD).\n",
        "\n",
        "2. Men exhibit a higher propensity for developing CHD compared to women.\n",
        "\n",
        "3. Smoking emerges as a risk factor for CHD, and the intensity of smoking plays a role in determining this risk.\n",
        "\n",
        "4. Individuals with high blood pressure, a history of stroke, or diabetes are at an elevated risk for CHD.\n",
        "\n",
        "5. Individuals with a history of stroke and prevalent hypertension are particularly prone to CHD risk.\n",
        "\n",
        "6. The presence of diabetes is also associated with an increased risk of CHD.\n",
        "\n",
        "7. Total cholesterol levels tend to be slightly higher among individuals at risk for CHD.\n",
        "\n",
        "8. Certain variables, such as age and systolic blood pressure, or BMI and glucose levels, exhibit positive associations, indicating interrelationships between these factors."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusions from Model Implementation:**\n",
        "\n",
        "1. Among the six models tested, the Random Forest Classifier and XGBoost models emerged as top performers, showcasing high accuracy, precision, and recall scores.\n",
        "\n",
        "2. While the KNN model displayed a relatively high recall score, its accuracy and precision scores fell behind those of the Random Forest Classifier and XGBoost models.\n",
        "\n",
        "3. The SVC model exhibited lower accuracy and ROC AUC scores, suggesting it might not be the most suitable choice for this specific classification task.\n",
        "\n",
        "4. The XGBoost model, with slightly higher test accuracy and precision scores compared to the Random Forest Classifier, along with a superior ROC AUC score, emerged as the preferred choice for predicting cardiovascular risk.\n",
        "\n",
        "5. Considering these results, the XGBoost model was selected as the optimal classification model for the cardiovascular risk prediction dataset, achieving an accuracy of 89.67%."
      ],
      "metadata": {
        "id": "odSQKcmc-K4T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "nA9Y7ga8ng1Z",
        "dauF4eBmngu3",
        "XbKrihe1hzIz",
        "gE9m4tYNhzI0",
        "u1JyNfSKhzI0",
        "re47_QOuhzI1",
        "oP4FUcWbhzI2",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "EXh0U9oCveiU",
        "0zABgQ6fOuwo",
        "LcXeDxriOuwo",
        "HWgkz62wOuwp",
        "eYYGfRzLOuwq",
        "wIm1WUAPOuwq",
        "fpeHKQKYOuwq",
        "7XtsEs2AOuws",
        "oJCdTSn6Ouws",
        "jGDnwRt0Ouwt",
        "d2SiN5Y6Ouwt",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "-oLEiFgy-5Pf",
        "pEMng2IbBLp7",
        "TNVZ9zx19K6k",
        "rMDnDkt2B6du",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "BhH2vgX9EjGr",
        "gjPt9TxnHPKA",
        "8zjU3qDCHPKN",
        "487lOzpVlRTM",
        "8If5zWZKpTaU",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}